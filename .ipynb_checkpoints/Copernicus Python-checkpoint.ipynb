{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import urllib\n",
    "import os\n",
    "import requests\n",
    "import shutil\n",
    "from sentinelsat import SentinelAPI, read_geojson, geojson_to_wkt\n",
    "from multiprocessing.pool import ThreadPool\n",
    "import time\n",
    "import geopandas as gpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Folders\n",
    "data_folder = os.path.abspath(\"data\")\n",
    "output_folder = os.path.abspath(\"output\")\n",
    "#check if outfolder exists if not create it\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Credentials\n",
    "username = \"myuser\"\n",
    "password = \"mypassword\"\n",
    "\n",
    "# Declarables\n",
    "url = \"https://scihub.copernicus.eu/apihub/odata/v1\"\n",
    "searchapi = \"https://scihub.copernicus.eu/dhus/search\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Helping Functions\n",
    "\n",
    "# Authenticate\n",
    "session_store = {}\n",
    "\n",
    "global multicred\n",
    "\n",
    "# MultiCredentials\n",
    "def init_session():\n",
    "    global multicred\n",
    "    multicred = pd.read_csv('credentials.csv').to_dict(orient='records')\n",
    "    #multicred = multicred[25]\n",
    "    parallelfactor = 2\n",
    "    for i, j in enumerate(multicred):\n",
    "        session = get_search_api_session(i)\n",
    "        if session.get('https://scihub.copernicus.eu/dhus/search?q=*').status_code != 200:\n",
    "            del multicred[i]\n",
    "        else:\n",
    "            print('----Successfully Validated Credential:', multicred[i])\n",
    "    for i in list(range(1, parallelfactor)):\n",
    "        multicred+=multicred\n",
    "    print(\"Total\", len(multicred), \"Connections Available for Download\")\n",
    "    #return multicred\n",
    "\n",
    "\n",
    "def get_search_api_session(i=0):\n",
    "    global multicred\n",
    "    #Configuration\n",
    "    username = multicred[i]['username']\n",
    "    password = multicred[i]['password']\n",
    "    try:\n",
    "        session = session_store[i]\n",
    "        if session.get('https://scihub.copernicus.eu/dhus/search?q=*').status_code != 200:\n",
    "            raise ValueError\n",
    "        else:\n",
    "            print('----using previous session')\n",
    "    except :\n",
    "        print('----creating session with username', username)\n",
    "        session = requests.Session()\n",
    "        session.auth = (username, password)\n",
    "        auth = session.get(searchapi)\n",
    "        session_store[i] = session\n",
    "    return session\n",
    "\n",
    "def download_scenes_multi(chunk):\n",
    "    multi_chunk_size = len(chunk)\n",
    "    chunk = chunk.reset_index().drop(columns=['index'])\n",
    "    chunk = chunk.reset_index()\n",
    "    print(\"\\n----Downloading %s Scenes\" % multi_chunk_size)\n",
    "    start = time.time()\n",
    "    pool = ThreadPool(multi_chunk_size)\n",
    "    for i in pool.imap_unordered(download_scenes, chunk.to_dict(orient='records')):\n",
    "        #print i\n",
    "        pass\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    print ('----All Files Downloaded Time elapsed: %s' % (time.time() - start))\n",
    "\n",
    "def download_scenes(row):\n",
    "    filename = row['title'] + \".zip\"\n",
    "    filepath = os.path.join(output_folder, filename)\n",
    "    url = row['link'].replace('apihub', 'dhus')\n",
    "    if not os.path.exists(filepath):\n",
    "        print(\"\\n----Downloading File at Url:\", url, \" | Session\", multicred[row['index']])\n",
    "        session = get_search_api_session(row['index'])\n",
    "        with session.get(url, stream=True) as r:\n",
    "            with open(filepath, 'wb') as f:\n",
    "                shutil.copyfileobj(r.raw, f, 10240)\n",
    "                #for chunk in r.iter_content(chunk_size=10240):\n",
    "                #    f.write(chunk)\n",
    "        print(\"\\n----File successfuly downloaded to \", filepath, \" | Url:\", url, \" | Session\", multicred[row['index']])\n",
    "    else:\n",
    "        print(\"\\n----Using Previouslly downloaded file \", filepath)\n",
    "    return filepath\n",
    "\n",
    "# Cleanup\n",
    "def cleanup():\n",
    "    output_files = os.listdir(output_folder)\n",
    "    output_files = [os.path.join(output_folder, x) for x in output_files if x.split('.')[-1].upper() == 'ZIP']\n",
    "    for i in output_files:\n",
    "        size = os.stat(i).st_size\n",
    "        if size < 1000:\n",
    "            os.remove(i)\n",
    "            print(\"----Invalid File\", i, \" with size \", size, \" bytes deleted\")\n",
    "        else:\n",
    "            print(\"----#Valid File\", i, \" with size \", (size/1024/1024), \" MB\")\n",
    "            \n",
    "# Initialize sessions\n",
    "init_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Area of interest\n",
    "aoiFilePath = os.path.join(data_folder, 'Ap_Sentinel_Brow.shp')\n",
    "aoiDf = gpd.read_file(aoiFilePath)\n",
    "aoiDf = aoiDf.to_crs(epsg =4326)\n",
    "aoi = aoiDf.unary_union.simplify(0.1)\n",
    "aoiWkt = aoi.wkt\n",
    "aoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to API\n",
    "api = SentinelAPI(username, password)\n",
    "\n",
    "# Search for Data\n",
    "#footprint = geojson_to_wkt(read_geojson('search_polygon.geojson'))\n",
    "#footprint = \"POLYGON ((79.2076887512349 15.7097283184072,81.5605925962714 15.7097283184072,81.5605925962714 17.1532438033589,79.2076887512349 17.1532438033589,79.2076887512349 15.7097283184072))\"\n",
    "products = api.query(aoiWkt,\n",
    "                     producttype='S2MSI2A',\n",
    "                     platformname='Sentinel-2',\n",
    "                     beginposition='[2020-01-01T00:00:00.000Z TO 2020-02-01T00:00:00.000Z]',\n",
    "                    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show Products data\n",
    "product_df = api.to_geodataframe(products)\n",
    "product_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to File\n",
    "time_columns = product_df.loc[:, product_df.dtypes == 'datetime64[ns]'].columns\n",
    "for x in time_columns:\n",
    "    product_df[x] = product_df[x].apply(lambda x: x.strftime('%Y-%m-%d'))\n",
    "product_df.to_file(driver='ESRI Shapefile', filename=os.path.join(output_folder, 'products_ap.shp'))\n",
    "product_df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Previous Files\n",
    "cleanup()\n",
    "output_files = os.listdir(output_folder)\n",
    "output_files = [ x[:-4] for x in output_files if x.split('.')[-1].upper() == 'ZIP']\n",
    "if len(output_files) > 0:\n",
    "    odf = pd.DataFrame(output_files)\n",
    "    download_df = product_df[~product_df['title'].isin(odf[0])]\n",
    "    print(\"Using \", (product_df.shape[0] - download_df.shape[0]), \" already saved files\")\n",
    "else:\n",
    "    download_df = product_df\n",
    "\n",
    "# Download Data\n",
    "#download_df = download_df.head(60)\n",
    "#files = os.listdir(output_folder)\n",
    "#files\n",
    "#product_df['saved_file'] = product_df.apply(download_scenes, axis=1)\n",
    "chunk_size = len(multicred)\n",
    "print('Downloading', len(download_df), 'files using', chunk_size, 'parallel downloads')\n",
    "df_chunks = download_df.groupby(np.arange(len(download_df))//chunk_size)\n",
    "for k,_df in df_chunks:\n",
    "    pass\n",
    "    download_scenes_multi(_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
